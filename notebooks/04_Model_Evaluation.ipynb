

# --------------------------
# 1️⃣ Imports & Load Data
# --------------------------

import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_recall_curve
import seaborn as sns
import matplotlib.pyplot as plt
import joblib  # to load saved models

from scripts.utils import print_separator

# Load processed data
df = pd.read_csv('../data/processed_transactions.csv')

X = df.drop(['transaction_id', 'customer_id', 'transaction_time', 'merchant', 'is_fraud'], axis=1)
y = df['is_fraud']

# --------------------------
# 2️⃣ Load Trained Models
# --------------------------

# Example: If you saved models as .pkl in Model Training Notebook
rf_model = joblib.load('../models/random_forest.pkl')
xgb_model = joblib.load('../models/xgboost.pkl')
logistic_model = joblib.load('../models/logistic_regression.pkl')

# --------------------------
# 3️⃣ Make Predictions
# --------------------------

rf_preds = rf_model.predict(X)
xgb_preds = xgb_model.predict(X)
logistic_preds = logistic_model.predict(X)

# --------------------------
# 4️⃣ Evaluate Each Model
# --------------------------

def evaluate_model(name, y_true, y_pred):
    print_separator(name)
    print(classification_report(y_true, y_pred))
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
    roc = roc_auc_score(y_true, y_pred)
    print(f"ROC-AUC Score: {roc:.4f}")

# Logistic Regression
evaluate_model("Logistic Regression", y, logistic_preds)

# Random Forest
evaluate_model("Random Forest", y, rf_preds)

# XGBoost
evaluate_model("XGBoost", y, xgb_preds)

# --------------------------
# 5️⃣ ROC & PR Curve Example
# --------------------------

# You can also plot ROC for one model
fpr, tpr, thresholds = roc_curve(y, rf_model.predict_proba(X)[:,1])
plt.plot(fpr, tpr, label='Random Forest')
plt.plot([0,1], [0,1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Random Forest')
plt.legend()
plt.show()

# Precision-Recall Curve
precision, recall, thresholds = precision_recall_curve(y, rf_model.predict_proba(X)[:,1])
plt.plot(recall, precision, label='Random Forest')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - Random Forest')
plt.show()
